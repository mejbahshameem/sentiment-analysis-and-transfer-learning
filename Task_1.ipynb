{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Task 1 - 2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heshumi/NNTI-WS2021-NLP-Project/blob/main/Task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VZXi_KGi0UR"
      },
      "source": [
        "# Task 1: Word Embeddings (10 points)\n",
        "\n",
        "This notebook will guide you through all steps necessary to train a word2vec model (Detailed description in the PDF)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48t-II1vkuau"
      },
      "source": [
        "## Imports\n",
        "\n",
        "This code block is reserved for your imports. \n",
        "\n",
        "You are free to use the following packages: \n",
        "\n",
        "(List of packages)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kh6nh84-AOL"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5mzQCBPTmas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec230920-fab7-4e7d-debd-479046227233"
      },
      "source": [
        "torch.manual_seed(21) # set randomness"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1e6a82b9d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWmk3hVllEcU"
      },
      "source": [
        "# 1.1 Get the data (0.5 points)\n",
        "\n",
        "The Hindi portion HASOC corpus from [github.io](https://hasocfire.github.io/hasoc/2019/dataset.html) is already available in the repo, at data/hindi_hatespeech.tsv . Load it into a data structure of your choice. Then, split off a small part of the corpus as a development set (~100 data points).\n",
        "\n",
        "If you are using Colab the first two lines will let you upload folders or files from your local file system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0I2LAhJv0wq"
      },
      "source": [
        "# #TODO: implement!\n",
        "\n",
        "# data = pd.read_csv('hindi_dataset.tsv', sep='\\t', usecols=['text'])\n",
        "# dev = data #.iloc[:100,]\n",
        "# dev.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6X9dfMraWAc"
      },
      "source": [
        "dev = pd.read_csv('bengali_hatespeech_sampled.csv', usecols=['text', 'task_1'])#.iloc[:100,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-mSJ8nUlupB"
      },
      "source": [
        "## 1.2 Data preparation (0.5 + 0.5 points)\n",
        "\n",
        "* Prepare the data by removing everything that does not contain information. \n",
        "User names (starting with '@') and punctuation symbols clearly do not convey information, but we also want to get rid of so-called [stopwords](https://en.wikipedia.org/wiki/Stop_word), i. e. words that have little to no semantic content (and, but, yes, the...). Hindi stopwords can be found [here](https://github.com/stopwords-iso/stopwords-hi/blob/master/stopwords-hi.txt) Then, standardize the spelling by lowercasing all words.\n",
        "Do this for the development section of the corpus for now.\n",
        "\n",
        "* What about hashtags (starting with '#') and emojis? Should they be removed too? Justify your answer in the report, and explain how you accounted for this in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHcNeyKi-AOQ"
      },
      "source": [
        "#TODO: implement!\n",
        "\n",
        "with open('stopwords-hi.txt', 'r') as f:\n",
        "  stopwords_hi = f.read().split()\n",
        "\n",
        "with open('stopwords-bn.txt', 'r') as f:\n",
        "  stopwords_bn = f.read().split()\n",
        "\n",
        "with open('stopwords-en.txt', 'r') as f:\n",
        "  stopwords = set(f.read().split() + stopwords_hi + stopwords_bn)\n",
        "\n",
        "punct = ':;?!-—-\\\"\\'|।()[]{},./\\\\“'\n",
        "\n",
        "def preprocess(x):\n",
        "  for ch in punct:\n",
        "    x = x.replace(ch, ' ')\n",
        "\n",
        "  words = x.split(' ')\n",
        "\n",
        "  preprocessed=[]\n",
        "\n",
        "  for word in words: \n",
        "    word = word.lower().strip()\n",
        "    if 2 < len(word) < 30 and word[0]!='@' and word not in stopwords and not word.startswith('http'): \n",
        "      preprocessed.append(word)\n",
        "\n",
        "  if len(preprocessed) == 0:\n",
        "    return np.nan\n",
        "\n",
        "  return preprocessed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liX7dUCX4Fl_"
      },
      "source": [
        "We chose to leave the emojis and hashtags in the text because they might indicate the user's mood which we will be trying to predict. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS3t0pJU5VDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff149cb-f215-46f9-82c9-ff1b32fabf99"
      },
      "source": [
        "dev['text'] = dev['text'].apply(lambda x: preprocess(x))\n",
        "dev = dev.dropna()\n",
        "dev = dev.reset_index(drop=True)\n",
        "\n",
        "dev['text'][:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [বিশ্বাস, ভাই, হাজারের, কমেন্টস, দেখলাম, বিচার...\n",
              "1             [ভাই, এসব, কথা, ভালো, আপনে, মানুষ, কিচো]\n",
              "2    [কাওয়, কাদের, মরলে, কুততা, যবে, জানোয়ার, দের, ...\n",
              "3    [এখনো, পদে, বহাল, কেনো, জাতি, চাই, জামালপুরের,...\n",
              "4                               [একটা, ছাগলের, বাচ্চা]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je09nozLmmMm"
      },
      "source": [
        "## 1.3 Build the vocabulary (0.5 + 0.5 points)\n",
        "\n",
        "The input to the first layer of word2vec is an one-hot encoding of the current word. The output of the model is then compared to a numeric class label of the words within the size of the skip-gram window. Now\n",
        "\n",
        "* Compile a list of all words in the development section of your corpus and save it in a variable ```V```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpoGmTKx-AOQ"
      },
      "source": [
        "#TODO: implement!\n",
        "V = {}\n",
        "\n",
        "for s in dev['text']:\n",
        "  for w in s:\n",
        "    if w in V:\n",
        "      V[w]+=1\n",
        "    else: \n",
        "      V[w]=1\n",
        "      \n",
        "summ = sum(V.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiaVglVNoENY"
      },
      "source": [
        "* Then, write a function ```word_to_one_hot``` that returns a one-hot encoding of an arbitrary word in the vocabulary. The size of the one-hot encoding should be ```len(v)```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqPNw6IT-AOQ"
      },
      "source": [
        "# TODO: implement!\n",
        "\n",
        "# # 1. Create a dictionary \"{word:1-hot} for a faster access\n",
        "# # Use this implementation for testing the captured semantics\n",
        "\n",
        "onehot_dict={}\n",
        "\n",
        "N = len(V)\n",
        "for i, word in enumerate(V.keys()):\n",
        "  onehot_dict[word] = np.append(np.append(np.zeros(i), [1]), np.zeros(N-i-1))\n",
        "\n",
        "def word_to_one_hot(word):\n",
        "  return onehot_dict[word]\n",
        "\n",
        "\n",
        "# # 2. Create a dictionary \"{word:word_num} and create the vectors on the go \n",
        "# # for memory economy. Use for training on large data\n",
        "\n",
        "# onehot_dict={}\n",
        "\n",
        "# N = len(V)\n",
        "# for i, word in enumerate(V.keys()):\n",
        "#   onehot_dict[word] = i\n",
        "\n",
        "# def word_to_one_hot(word):\n",
        "#   i = onehot_dict[word]\n",
        "#   return np.append(np.append(np.zeros(i), [1]), np.zeros(N-i-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwbfjwsCkpue"
      },
      "source": [
        "# for key, item in onehot_dict.items():\n",
        "#   if item[-1] == 1:\n",
        "#     print (key, len(item))\n",
        "#     print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvFJu59Yk_t1",
        "outputId": "1b1f3e43-3fe7-48e4-c787-c1ac40e31dc6"
      },
      "source": [
        "len(onehot_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14324"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7yEUOQVfK27"
      },
      "source": [
        "# onehot_dict['डालें']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKD8zBlxVclh"
      },
      "source": [
        "## 1.4 Subsampling (0.5 points)\n",
        "\n",
        "The probability to keep a word in a context is given by:\n",
        "\n",
        "$P_{keep}(w_i) = \\Big(\\sqrt{\\frac{z(w_i)}{0.001}}+1\\Big) \\cdot \\frac{0.001}{z(w_i)}$\n",
        "\n",
        "Where $z(w_i)$ is the relative frequency of the word $w_i$ in the corpus. Now,\n",
        "* Calculate word frequencies\n",
        "* Define a function ```sampling_prob``` that takes a word (string) as input and returns the probabiliy to **keep** the word in a context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj4sDOVMMr0b"
      },
      "source": [
        "#TODO: implement!\n",
        "\n",
        "def sampling_prob(word):\n",
        "  z = V[word]/summ\n",
        "\n",
        "  return (sqrt(z/0.001)+1)*0.001/z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxV1P90zplxu"
      },
      "source": [
        "# 1.5 Skip-Grams (1 point)\n",
        "\n",
        "Now that you have the vocabulary and one-hot encodings at hand, you can start to do the actual work. The skip gram model requires training data of the shape ```(current_word, context)```, with ```context``` being the words before and/or after ```current_word``` within ```window_size```. \n",
        "\n",
        "* Have closer look on the original paper. If you feel to understand how skip-gram works, implement a function ```get_target_context``` that takes a sentence as input and [yield](https://docs.python.org/3.9/reference/simple_stmts.html#the-yield-statement)s a ```(current_word, context)```.\n",
        "\n",
        "* Use your ```sampling_prob``` function to drop words from contexts as you sample them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8CCTpVy-AOR"
      },
      "source": [
        "#TODO: implement!\n",
        "\n",
        "def get_target_context(sentence, window_size=5):\n",
        "  new_sent = [w for w in sentence if random.random() < sampling_prob(w)] # Remove too frequent words \n",
        "\n",
        "  for i, w in enumerate(new_sent): # For each word in a sentence (or comment)\n",
        "    context = [y for y in new_sent[i-window_size:i] + new_sent[i+1:i+window_size+1]]\n",
        "    yield (w, context) # yield its context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfEFgtkmuDjL"
      },
      "source": [
        "# 1.6 Hyperparameters (0.5 points)\n",
        "\n",
        "According to the word2vec paper, what would be a good choice for the following hyperparameters? \n",
        "\n",
        "* Embedding dimension\n",
        "* Window size\n",
        "\n",
        "Initialize them in a dictionary or as independent variables in the code block below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7xSKuFJcYoD"
      },
      "source": [
        "# Set hyperparameters\n",
        "window_size = 5\n",
        "embedding_size = 300\n",
        "input_size = len(V) # Onehot vector length\n",
        "batch_size = 100\n",
        "\n",
        "# More hyperparameters\n",
        "learning_rate = 0.05\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bheJUy-jQOi8"
      },
      "source": [
        "# Create a dataset and a data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFYvDTrEKZUo"
      },
      "source": [
        "class HindiDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx): # Returns pairs of 'word - context' in onehot\n",
        "    word_list=[] # List of input words\n",
        "    cont_list=[] # List of corresponding context words\n",
        "    \n",
        "    for word, context in get_target_context(self.df[idx], window_size= window_size):\n",
        "        word_onehot = word_to_one_hot(word)\n",
        "\n",
        "        for cont_word in context:\n",
        "          cont_onehot = word_to_one_hot(cont_word)\n",
        "\n",
        "          word_list.append(word_onehot)\n",
        "          cont_list.append(cont_onehot)\n",
        "    \n",
        "    word_list = torch.tensor(word_list) #.type(torch.DoubleTensor)\n",
        "    cont_list = torch.tensor(cont_list)\n",
        "\n",
        "    wc = torch.stack([word_list, cont_list])\n",
        "    wc.type(torch.DoubleTensor)\n",
        "    return wc # 2 columns: word_vector tensor, context_vector tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVK9v-llnp2m"
      },
      "source": [
        "def my_collate(batches): # batches is a list of tensors returned from __getitem()__\n",
        "  data = []\n",
        "  labels = []\n",
        "  for b in batches:\n",
        "    data.append(b[0])\n",
        "    labels.append(b[1])\n",
        "\n",
        "  d = torch.cat(data, dim=0) # data\n",
        "  l = torch.cat(labels, dim=0) # labels\n",
        "  \n",
        "  return (d, l)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7jZgGGUXuta"
      },
      "source": [
        "dataset = HindiDataset(dev['text'])\n",
        "dataloader = DataLoader(dataset, collate_fn = my_collate, batch_size= batch_size, shuffle = False, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbQQn0eeJw1Q",
        "outputId": "0c0a62bd-a5bb-42c2-b338-8a3e65d58a77"
      },
      "source": [
        "len(V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14324"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRuBx_3-4uJu"
      },
      "source": [
        "## Create files of tensors for a faster access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE9w_tO546Iv"
      },
      "source": [
        "# for i, batch in enumerate(dataloader):\n",
        "#   full_batch = torch.stack(batch).type(torch.int8)\n",
        "#   print(full_batch.size())\n",
        "#   print(full_batch.size()[0]*full_batch.size()[1]*full_batch.size()[2]*8 *1.25e-10)\n",
        "#   torch.save(batch[0], 'Dataset/batch-test{}.pt'.format(i))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiM2zq-YunPx"
      },
      "source": [
        "# 1.7 Pytorch Module (0.5 + 0.5 + 0.5 points)\n",
        "\n",
        "Pytorch provides a wrapper for your fancy and super-complex models: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The code block below contains a skeleton for such a wrapper. Now,\n",
        "\n",
        "* Initialize the two weight matrices of word2vec as fields of the class.\n",
        "\n",
        "* Override the ```forward``` method of this class. It should take a one-hot encoding as input, perform the matrix multiplications, and finally apply a log softmax on the output layer.\n",
        "\n",
        "* Initialize the model and save its weights in a variable. The Pytorch documentation will tell you how to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9sGNytYhwxS"
      },
      "source": [
        "# Create model \n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # Hidden layer\n",
        "    self.fc1 = nn.Linear(input_size, embedding_size)\n",
        "\n",
        "    # Output layer\n",
        "    self.fc2 = nn.Linear(embedding_size, input_size)\n",
        "\n",
        "  def forward(self, one_hot):\n",
        "    x = self.fc1(one_hot)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    \n",
        "    # Softmax is not needed in cosine similarity as the vectors are normalized\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K47V_uueSa0o",
        "outputId": "5257d0bb-049e-4b89-e0dd-88841aa94aea"
      },
      "source": [
        "model = Word2Vec()\n",
        "\n",
        "model.fc1.weight.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([300, 14324])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfU62gX8k7lT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14eb0f67-b546-49c9-bbf2-a2ee49b6e543"
      },
      "source": [
        "torch.manual_seed(21) # For reproducibility\n",
        "\n",
        "model = Word2Vec()\n",
        "model.double()\n",
        "# model.cuda()\n",
        "\n",
        "nn.init.uniform_(model.fc1.weight)\n",
        "nn.init.uniform_(model.fc2.weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[0.0668, 0.4083, 0.8056,  ..., 0.8653, 0.0113, 0.2629],\n",
              "        [0.4388, 0.4043, 0.6394,  ..., 0.7573, 0.1224, 0.9858],\n",
              "        [0.7768, 0.8394, 0.6113,  ..., 0.6657, 0.3316, 0.6482],\n",
              "        ...,\n",
              "        [0.1821, 0.0859, 0.3567,  ..., 0.5399, 0.0650, 0.7519],\n",
              "        [0.8096, 0.1359, 0.5630,  ..., 0.5545, 0.0962, 0.9322],\n",
              "        [0.1502, 0.6163, 0.8451,  ..., 0.2649, 0.9688, 0.9619]],\n",
              "       dtype=torch.float64, requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XefIDMMHv5zJ"
      },
      "source": [
        "# 1.8 Loss function and optimizer (0.5 points)\n",
        "\n",
        "Initialize variables with [optimizer](https://pytorch.org/docs/stable/optim.html#module-torch.optim) and loss function. You can take what is used in the word2vec paper, but you can use alternative optimizers/loss functions if you explain your choice in the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9-Ino-e29w3"
      },
      "source": [
        "# Define optimizer and loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "criterion = nn.CosineEmbeddingLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckTfK78Ew8wI"
      },
      "source": [
        "# 1.9 Training the model (3 points)\n",
        "\n",
        "As everything is prepared, implement a training loop that performs several passes of the data set through the model. You are free to do this as you please, but your code should:\n",
        "\n",
        "* Load the weights saved in 1.6 at the start of every execution of the code block\n",
        "* Print the accumulated loss at least after every epoch (the accumulate loss should be reset after every epoch)\n",
        "* Define a criterion for the training procedure to terminate if a certain loss value is reached. You can find the threshold by observing the loss for the development set.\n",
        "\n",
        "You can play around with the number of epochs and the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbMGD5L0mLDx"
      },
      "source": [
        "# Define train procedure\n",
        "\n",
        "# load initial weights\n",
        "import time \n",
        "\n",
        "def train():\n",
        "  \n",
        "  print(\"Training started\")\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    acc_loss=0\n",
        "    words_num=0\n",
        "    t1=time.time()\n",
        "    for i, batch in enumerate(dataloader): # This line is slow for large data\n",
        "      # t2 = time.time()\n",
        "      # t3 = t2 - t1\n",
        "\n",
        "\n",
        "      # print('Time for dataloader: {}'.format(t3))\n",
        "\n",
        "      word_t = batch[0]#.cuda()\n",
        "      cont_t = batch[1]#.cuda()\n",
        "      \n",
        "      if word_t.size()[0] == 0:\n",
        "        continue\n",
        "\n",
        "      outputs = model(word_t)\n",
        "\n",
        "      loss = criterion(cont_t, outputs, torch.ones(len(cont_t)))#.cuda()) # Correctly computes 1 - cos(a,b)\n",
        "    \n",
        "      acc_loss += loss.item()\n",
        "      words_num += 1\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # t1 = time.time()\n",
        "      # t4 = t1 - t2\n",
        "      \n",
        "      # print('Time for the rest: {} s'.format(t4))\n",
        "    \n",
        "    print('Epoch: {}; Accumulated mean loss: {}'.format(epoch+1, acc_loss/words_num))\n",
        "    torch.save(model.state_dict(), 'Word2Vec-bg.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiuuvQKQV-Lz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "06d1e356-0907-4aab-9bad-d1f18ee06287"
      },
      "source": [
        "train()\n",
        "\n",
        "print(\"Training finished\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b74eb644c155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c80bb50ab3c2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ROXRkvdRdi"
      },
      "source": [
        "Batch size = 200 => Error = 0.7 on the 100th epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4NBPgLiquXu"
      },
      "source": [
        "batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLb5OvBqrG_p"
      },
      "source": [
        "For batch_size = 1 the timing for 100 point is max: 0.03\n",
        "\n",
        "For batch_size = 1 the timing for all points is max: 0.43"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EuuxlWO84lh"
      },
      "source": [
        "Each batch is processed in 0.8 s maximum, however when dealing woth darge data, the batches are processed in 10 seconds each! The batches sized are the same.\n",
        "\n",
        "Why does it happen?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFcML10Crbvk"
      },
      "source": [
        "**Experiment Results**\n",
        "\n",
        "1) Window Size: 5; lr: 0.05; embedding_size = 30; subsampling=True\n",
        "\n",
        "Epoch: 1; Error: 0.915\n",
        "\n",
        "Epoch: 17; Error: 0.858 \n",
        "\n",
        "Epoch 50: Error 0.8499\n",
        "\n",
        "Epoch 70: Error 0.8473\n",
        "\n",
        "Epoch 90: Error 0.8486\n",
        "\n",
        "Epoch 100: Error 0.8478\n",
        "\n",
        "2) Window: 3, lr: 0.05; embedding_size = 300; subsampling = False\n",
        "\n",
        "Epoch 1: 0.906\n",
        "\n",
        "Epoch 7: 0.8496\n",
        "\n",
        "Epoch 25: 0.8129\n",
        "\n",
        "Epoch 50: 0.8194\n",
        "\n",
        "3) Window: 5, lr: 0.05, embedding = 300, subs = True, batch_size = 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgQkaYstyj0Q"
      },
      "source": [
        "# 1.10 Train on the full dataset (0.5 points)\n",
        "\n",
        "Now, go back to 1.1 and remove the restriction on the number of sentences in your corpus. Then, reexecute code blocks 1.2, 1.3 and 1.6 (or those relevant if you created additional ones). \n",
        "\n",
        "* Then, retrain your model on the complete dataset.\n",
        "\n",
        "* Now, the input weights of the model contain the desired word embeddings! Save them together with the corresponding vocabulary items (Pytorch provides a nice [functionality](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for this)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "OW3ASA-AsnWE",
        "outputId": "434abbdc-a340-4b39-882a-2255e6ef167e"
      },
      "source": [
        "train()\n",
        "\n",
        "print(\"Training finished\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training started\n",
            "Time for dataloader: 3.9300851821899414\n",
            "Time for the rest: 0.06310796737670898 s\n",
            "Time for dataloader: 3.6967005729675293\n",
            "Time for the rest: 0.05514931678771973 s\n",
            "Time for dataloader: 4.438810110092163\n",
            "Time for the rest: 0.0655677318572998 s\n",
            "Time for dataloader: 3.705820083618164\n",
            "Time for the rest: 0.05425596237182617 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-10e2d2619775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-d5cb676204f7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mwords_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# This line is slow for large data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mt3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-9d4f7985c0c8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcont_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5uxv7VDbH1p"
      },
      "source": [
        "Data size with 100 first points, Batch_size = 100: 13478\n",
        "\n",
        "Data size with all points, Batch_size = 100: 13617\n",
        "\n",
        "As it is expected to be. However, each batch is processed way slower, and the RAM consumption might be higher\n",
        "\n",
        "The training is continued on the cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXzW2vko-vpz"
      },
      "source": [
        "torch.save(model.state_dict(), 'Word2Vec.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiebM9BRHhqw"
      },
      "source": [
        "# All data experiments\n",
        "\n",
        "1) window_size = 5; embedding_size = 300; lr = 0.05;\n",
        "\n",
        "Epoch: 1; Accumulated mean loss: 0.9824172251430727\n",
        "\n",
        "Epoch: 2; Accumulated mean loss: 0.9769900029832989\n",
        "\n",
        "Epoch: 3; Accumulated mean loss: 0.9706227188189077\n",
        "\n",
        "Epoch: 4; Accumulated mean loss: 0.963010271279602\n",
        "\n",
        "Epoch: 5; Accumulated mean loss: 0.9569715874365621\n",
        "\n",
        "Epoch: 6; Accumulated mean loss: 0.9525484683097712\n",
        "\n",
        "Epoch: 7; Accumulated mean loss: 0.9490198400897474\n",
        "\n",
        "Epoch: 8; Accumulated mean loss: 0.9462296936510709\n",
        "\n",
        "Epoch: 9; Accumulated mean loss: 0.9439137938502523\n",
        "\n",
        "Epoch: 10; Accumulated mean loss: 0.9419612759007573\n",
        "\n",
        "Epoch: 11; Accumulated mean loss: 0.9401766861262544\n",
        "\n",
        "Epoch: 12; Accumulated mean loss: 0.9387034317395186\n",
        "\n",
        "Epoch: 13; Accumulated mean loss: 0.9373532943693524\n",
        "\n",
        "Epoch: 14; Accumulated mean loss: 0.936140513085175\n",
        "\n",
        "Epoch: 30; Accumulated mean loss: 0.9279157843358515\n",
        "\n",
        "Epoch: 50; Accumulated mean loss: 0.9227518338587359\n",
        "\n",
        "Epoch: 80; Accumulated mean loss: 0.9193855124162842\n",
        "\n",
        "Epoch: 90; Accumulated mean loss: 0.9186107575588937\n",
        "\n",
        "Epoch: 100; Accumulated mean loss: 0.918196428019041"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg6vnQQGsfnJ"
      },
      "source": [
        "# Debugging notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l3kzskoD-Jv",
        "outputId": "9f7ec59b-7e7a-4517-cf36-4573c1cea353"
      },
      "source": [
        "model = Word2Vec()#.cuda()\n",
        "model.double()\n",
        "\n",
        "model.load_state_dict(torch.load('Word2Vec-bg.model', map_location=torch.device('cpu')))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Word2Vec(\n",
              "  (fc1): Linear(in_features=14324, out_features=300, bias=True)\n",
              "  (fc2): Linear(in_features=300, out_features=14324, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbfZnMsJoV_O",
        "outputId": "8b3bf4f3-4f96-454f-dc62-22bca40f322b"
      },
      "source": [
        "# Outputs 10 most similar words to the input word. \n",
        "\n",
        "word = torch.from_numpy(word_to_one_hot('মুভির'))#.cuda() # देशद्रोही\n",
        "word = word.view(1, len(word))\n",
        "\n",
        "context = model(word)\n",
        "\n",
        "max = torch.topk(context, 10) # most suitable context (sorted by relevance)\n",
        "\n",
        "for idx in max[1][0]:\n",
        "  for w, onehot in onehot_dict.items():\n",
        "      if np.argmax(onehot) == idx:\n",
        "          print(w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "চুদার\n",
            "থাকি\n",
            "ভাইয়ের\n",
            "জানতাম\n",
            "সৌদি\n",
            "শুনতাম\n",
            "খুবই\n",
            "গান<br\n",
            "হয়না\n",
            ">হৃদয়\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbIWW6RTOvcz"
      },
      "source": [
        "**Works well!**"
      ]
    }
  ]
}